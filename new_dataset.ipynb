{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"data/00--raw/football-field-detection.v15i.yolov5pytorch/train/\"\n",
    "\n",
    "IMG_PATH = f\"{train_path}images/0a2d9b_2_3_png.rf.2b39030ff9f2e93a34aa9ca69abbd77c.jpg\"\n",
    "LBL_PATH = f\"{train_path}/labels/0a2d9b_2_3_png.rf.2b39030ff9f2e93a34aa9ca69abbd77c.txt\"\n",
    "\n",
    "IMG_PATH = f\"{train_path}images/0a2d9b_6_11_png.rf.2cfd6b6dad39a0f39eee5eb3d729823f.jpg\"\n",
    "LBL_PATH = f\"{train_path}/labels/0a2d9b_6_11_png.rf.2cfd6b6dad39a0f39eee5eb3d729823f.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from src.struct.shared_data   import SharedAnnotations\n",
    "from src.visual.field         import FieldVisualizer, PitchConfig\n",
    "from src.visual.visualizer    import Visualizer\n",
    "\n",
    "# 1) Roboflow → your index mapping\n",
    "rf2my = {\n",
    "     1:  0,    2:  7,   \n",
    "    11: 27, 12: 28,\n",
    "    13: 10,   4: 16,   \n",
    "     8: 18,  9:  5,\n",
    "     7: 17,  15: 23,   \n",
    "    16: 25, 17: 26,\n",
    "    10:  9,  26: 22,   \n",
    "    27:  1, 28: 11,\n",
    "    20: 13,  21: 29,   \n",
    "    22: 30, 23: 14,\n",
    "    24:  6,  29: 15,   \n",
    "    25: 21\n",
    "}\n",
    "\n",
    "rf2my2 = {\n",
    "    1:   0,\n",
    "    2:   7,\n",
    "    3:  15,\n",
    "    4:  16,\n",
    "    5:   8,\n",
    "    6:   2,\n",
    "    7:  17,\n",
    "    8:  18,\n",
    "    9:   5,\n",
    "    10:  9,\n",
    "    11: 27,\n",
    "    12: 28,\n",
    "    13: 10,\n",
    "    14:   -1,\n",
    "    15: 23, \n",
    "    16: 25,\n",
    "    17: 26,\n",
    "    18:   -1,\n",
    "    19:   -1,\n",
    "    20: 13, \n",
    "    21: 29,\n",
    "    22: 30, \n",
    "    23: 14,\n",
    "    24:  6,\n",
    "    25: 21,\n",
    "    26: 22,   \n",
    "    27:  1, \n",
    "    28: 11,\n",
    "    29: 15,   \n",
    "}\n",
    "\n",
    "# class  xc  yc  w  h   x0  y0  v0   x1  y1  v1   x2  y2  v2 ... xN yN vN\n",
    "\n",
    "# field     count       meaning                                     coordinate space\n",
    "# class\t    1\t        always 0 (only one class: “football field”)\t–\n",
    "# xc yc w h\t4\t        YOLO bounding-box (we normally ignore it)\tnormalised (0-1)\n",
    "# xi yi\t    2 × (N+1)\tposition of landmark i\t                    normalised (0-1)\n",
    "# vi\t    1 × (N+1)\tvisibility flag (0 = not labelled / occluded, 1 = labelled)\n",
    "\n",
    "\n",
    "# 2) load one example\n",
    "IMG = IMG_PATH\n",
    "LBL = LBL_PATH\n",
    "\n",
    "img = cv2.imread(IMG)\n",
    "h, w = img.shape[:2]\n",
    "\n",
    "# 3) parse Roboflow keypoints\n",
    "vals = list(map(float, open(LBL).read().split()))\n",
    "kp   = np.array(vals[5:], dtype=np.float32).reshape(-1,3)  # (num_pts, 3)\n",
    "\n",
    "# 4) init shared‐data and field\n",
    "shared    = SharedAnnotations()\n",
    "fv        = FieldVisualizer(PitchConfig(draw_points=True))\n",
    "model_pts = fv.get_hardcoded_model_points()  # shape (M,2)\n",
    "\n",
    "# 5) build correspondences\n",
    "pts_img   = []\n",
    "pts_model = []\n",
    "for j, (xn,yn,vis) in enumerate(kp):\n",
    "    if vis <= 0 or j not in rf2my:\n",
    "        continue\n",
    "    my_idx = rf2my[j]\n",
    "\n",
    "    # image‐space pixel\n",
    "    x_px, y_px = float(xn*w), float(yn*h)\n",
    "    shared.captured_video_pts.append((x_px, y_px))\n",
    "    pts_img.append([x_px, y_px])\n",
    "\n",
    "    # model‐space metre\n",
    "    mx, my = model_pts[my_idx]\n",
    "    shared.ground_truth_pts.append((mx, my))\n",
    "    pts_model.append([mx, my])\n",
    "\n",
    "pts_img   = np.array(pts_img,   dtype=np.float32)\n",
    "pts_model = np.array(pts_model, dtype=np.float32)\n",
    "\n",
    "if len(pts_img) < 4:\n",
    "    raise RuntimeError(f\"Need ≥4 matches, got {len(pts_img)}\")\n",
    "\n",
    "# 6) fit homography H: model→image\n",
    "H, _                 = cv2.findHomography(pts_model, pts_img, cv2.RANSAC, 3.0)\n",
    "shared.H_video2field = H\n",
    "\n",
    "# 7) set up the Visualizer\n",
    "class DummyProcess:\n",
    "    def __init__(self, sd):      self.shared_data = sd\n",
    "    def is_done(self):           return True\n",
    "    def on_mouse_click(self, x,y): pass\n",
    "\n",
    "viz = Visualizer(\n",
    "    field_config = PitchConfig(linewidth=2, draw_points=True),\n",
    "    frame        = img,             # initial video frame\n",
    "    class_names  = {}, \n",
    "    process      = DummyProcess(shared)\n",
    ")\n",
    "\n",
    "# prime the video side\n",
    "viz.video_visualizer.update(type(\"VF\",(object,),{\n",
    "    \"frame_id\":0, \"timestamp\":0.0,\n",
    "    \"image\":img,\n",
    "    \"detections\":[]\n",
    "})())\n",
    "\n",
    "# ─── EXPERIMENT: random image points → project to model ────────────────────\n",
    "num_rand = 12\n",
    "rand_px  = np.column_stack([\n",
    "    np.random.uniform(0, w, num_rand),\n",
    "    np.random.uniform(0, h, num_rand),\n",
    "])\n",
    "\n",
    "# draw these as blue dots on the video\n",
    "shared.sampled_video_pts = [(float(x), float(y)) for x,y in rand_px]\n",
    "\n",
    "# now project them into model space via H_inv\n",
    "H_inv = np.linalg.inv(shared.H_video2field)\n",
    "rand_model = viz.transform_points(rand_px, H_inv)\n",
    "\n",
    "# draw these as _model‐space_ blue dots on the field\n",
    "shared.projected_detection_model_pts = [\n",
    "    (float(mx), float(my)) for mx,my in rand_model\n",
    "]\n",
    "\n",
    "# ─── 8) annotate & render ─────────────────────────────────────────────────\n",
    "viz.video_visualizer.clear_annotations()\n",
    "viz.field_visualizer.clear_annotations()\n",
    "viz._annotate_frames(\n",
    "    viz.video_visualizer.frame,\n",
    "    viz.field_visualizer.frame\n",
    ")\n",
    "out = viz.generate_combined_view()\n",
    "\n",
    "# ─── 9) display ────────────────────────────────────────────────────────────\n",
    "display_scale = 0.8  # e.g. 50% of original size\n",
    "out_resized = cv2.resize(out, None, fx=display_scale, fy=display_scale, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "cv2.imshow(\"Random Projection Test\", out_resized)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
